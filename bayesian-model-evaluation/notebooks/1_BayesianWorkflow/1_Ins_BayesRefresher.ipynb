{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Bayesian refresher and introduction to ArviZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What does an end-to-end Bayesian workflow look like?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Refresh our understanding of Bayes' Theorem\n",
    "* Fit a small binomial model\n",
    "* Show how a full statistical workflow, even outside of Bayesian methods, requires more steps more than just model fitting\n",
    "* Introduce ArviZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "### The most common formulation\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "P(\\theta \\mid y) = \\frac{ P(y \\mid \\theta)p(\\theta)}{p(y)}\n",
    "$$\n",
    "\n",
    "This comes from a simple rearranging of terms for joint probabilities:\n",
    "\n",
    "$$\n",
    "P(\\theta, y) = P(\\theta)P(y | \\theta) = P(y)P(\\theta | y)\n",
    "$$\n",
    "\n",
    "This formula becomes interesting when we interpret $y$ as _data_ and $\\theta$ as _parameters_ for a model. \n",
    "\n",
    "### Breaking it down\n",
    "####  $P(\\theta)$ -> Prior\n",
    "_\"What is the probability of parameters given no observations\"_  \n",
    "Before we've observed any data what is a plausible probability distribution of parameters? This may come from physical constraints (temperatures are above 0 Kelvin), or domain expertise (high temperatures in Austin in summer are between 80 and 110).\n",
    "\n",
    "####  $P(y \\mid \\theta )$ -> Likelihood\n",
    "_\"What is the probability of the observed data given a model parameter value\"_  \n",
    "\n",
    "Likelihood functions tell us how \"likely\" the observed data is, for all the possible parameter values. Likelihoods perform roughly the same role as loss functions from \"machine learning\": evaluating how \"good\" of a set of model parameters are at explaining the data. Indeed, many common loss functions are derived from likelihoods.\n",
    "\n",
    "####  $P(\\theta \\mid y)$ -> Posterior distribution\n",
    "_\"What is the distribution of parameters given the observed data?\"_  \n",
    "\n",
    "After obtaining data, or making observations, what is our belief regarding the parameters of the underlying statistical model? \n",
    "\n",
    "* Estimating the posterior distribution is the goal of Bayesian analysis. \n",
    "* The process of estimating the posterior distribution often referred to as **Inference**\n",
    "* There are numerous ways to perform inference, [each with their own pros and cons](http://canyon289.github.io/pages/InferenceCheatsheet.html)\n",
    "    * In this tutorial we will only be using Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "####  $P(y)$ -> Marginal Probability of Evidence\n",
    "_\"What is the probability distribution of data?_\n",
    "\n",
    "In most cases this term is difficult or impossible to calculate, so much so that most inference techniques cleverly get around their calculations. MCMC is one of those techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative formulations\n",
    "\n",
    "### Likelihood notation\n",
    "I particularly like this formulation because clearly demarcates difference between Likelihood and probability terms  \n",
    "\n",
    "$$ P(\\theta | y) = \\frac{ L(\\theta | y)p(\\theta)}{p(y)} $$\n",
    "\n",
    "### Defined as a proportion\n",
    "While the posterior, likelihood, and prior are usually *distributions*, the denominator is a scalar that normalizes the numerator. In many modern Bayesian Inference Methodswe try to avoid calculating this\n",
    "\n",
    "$$ P(\\theta | y) \\propto P(y | \\theta)p(\\theta) $$\n",
    "\n",
    "### Defined with puppies\n",
    "Even if you hate math, you'd have to be a monster to hate puppies. This pictorial formula is taken from  John Kruschke's excellent book [Doing Bayesian Data Analysis](https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884) Do note the lazy puppy on the right. The laziness is an indication of how little work this puppy does in most Bayesian Inference methods.  \n",
    "![BayesianPuppies](../../img/Doing-DBA.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
